{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Tent Functions for ML\n",
    "\n",
    "This is a demonstration of how to use tent functions for machine learning on a set of persistence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from teaspoon.ML.Base import ParameterBucket, build_G, TentParameters, train_test_split, ML_via_featurization\n",
    "from teaspoon.MakeData import PointCloud as gpc\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Diagrams from Manifold Test\n",
    "\n",
    "Generate persistence diagrams drawn from random point clouds of a torus, annulus, cube, three clusters, three clusters of three clusters, and spheres. See details of *testSetManifolds* function [here](http://elizabethmunch.com/code/teaspoon/namespaceteaspoon_1_1_make_data_1_1_point_cloud.html#a5d9c892f9f0a63f64437cbbde9048aeb). Select the dimension of persistence diagram to use, here we use dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating torus clouds...\n",
      "Generating annuli clouds...\n",
      "Generating cube clouds...\n",
      "Generating three cluster clouds...\n",
      "Generating three clusters of three clusters clouds...\n",
      "Generating sphere clouds...\n",
      "Finished generating clouds and computing persistence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = gpc.testSetManifolds(numDgms = 100, numPts = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split & Set up Parameter Bucket\n",
    " \n",
    "### Train Test Split:\n",
    " - Choose which column (or columns) you want to use for diagrams\n",
    " - Specify which column has the training labels\n",
    " \n",
    "### Parameter Bucket\n",
    " - Need a TentParameters parameter bucket. \n",
    " - Set parameter d for mesh size in each direction.\n",
    " - Get adaptive partitions. \n",
    " - Set delta and epsilon for each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the data to ordinal...\n",
      "Uh oh your support will cross the diagonal, your bottom boundary is  -0.7095354720950127\n",
      "Shifting the boundary of the partition up by necessary amount...\n",
      "\n",
      "Partitions d, delta and epsilon have all been assigned to each partition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dgm_col = ['Dgm1']\n",
    "labels_col = ['trainingLabel']\n",
    "\n",
    "# Set up parameters and adaptively partition training set\n",
    "params = TentParameters()\n",
    "params.useAdaptivePart == False\n",
    "\n",
    "# Run train/test split using sklearn\n",
    "D_train, D_test, L_train,L_test = train_test_split(df, df[labels_col], test_size=params.test_size, random_state = params.seed)\n",
    "\n",
    "# Get the portions of the test data frame with diagrams and concatenate into giant series:\n",
    "allDgms = pd.concat((D_train[label] for label in dgm_col))\n",
    "\n",
    "if params.useAdaptivePart == True:\n",
    "    # Hand the series to the makeAdaptivePartition function\n",
    "    params.d = [3,3]\n",
    "    params.makeAdaptivePartition(allDgms, meshingScheme = 'DV', numParts = 3)\n",
    "else:\n",
    "    # Just use the bounding box as the partition\n",
    "    params.d = 3\n",
    "    params.makeAdaptivePartition(allDgms, meshingScheme = 'None')\n",
    "    \n",
    "# Assign delta and epsilon for each partition\n",
    "# If you didn't use adaptive partitioning this just assigns it to the one partition for the whole bounding box\n",
    "params.chooseDeltaEpsForPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Use function from teaspoon to run ML with featurization on persistence diagrams. Takes data frame of persistence diagrams and specified column labels, computes the G matrix using *build_G*. Does classification using labels from labels_col in the data frame. Returns trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 402/600 to train...\n",
      "Training estimator.\n",
      "Making G...\n",
      "Number of features used is 16 ...\n",
      "Checking score on training set...\n",
      "Score on training set: 0.8681592039800995.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Using ' + str(len(L_train)) + '/' + str(len(df)) + ' to train...')\n",
    "clf = ML_via_featurization(D_train, labels_col = labels_col[0], dgm_col = dgm_col, params = params, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Build G matrix for the testing set, use the model generated on the training data to predict the label. Then score the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 198/600 to test...\n",
      "Score on testing set: 0.8787878787878788...\n",
      "\n",
      "Finished with train/test experiment.\n"
     ]
    }
   ],
   "source": [
    "#--------Testing-------------#\n",
    "print('Using ' + str(len(L_test)) + '/' + str(len(df)) + ' to test...')\n",
    "listOfG = []\n",
    "for dgmColLabel in dgm_col:\n",
    "    G = build_G(D_test[dgmColLabel],params)\n",
    "    listOfG.append(G)\n",
    "\n",
    "G = np.concatenate(listOfG,axis = 1)\n",
    "\n",
    "# Compute predictions and add to DgmsDF data frame\n",
    "L_predict = pd.Series(clf.predict(G),index = L_test.index)\n",
    "df['Prediction'] = L_predict\n",
    "\n",
    "# Compute score\n",
    "score = clf.score(G,list(L_test['trainingLabel']))\n",
    "\n",
    "print('Score on testing set: ' + str(score) +\"...\\n\")\n",
    "\n",
    "print('Finished with train/test experiment.')\n",
    "\n",
    "output = {}\n",
    "output['score'] = score\n",
    "output['DgmsDF'] = df\n",
    "output['clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
